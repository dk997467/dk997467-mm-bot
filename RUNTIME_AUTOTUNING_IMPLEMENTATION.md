# Runtime Auto-Tuning Implementation

**Status:** âœ… COMPLETE  
**Date:** 2025-10-12  
**Prompt:** D â€” Runtime auto-tuning Ð² Soak: on-the-fly Ð¿Ð¾Ð´ÑÑ‚Ñ€Ð¾Ð¹ÐºÐ° Ð¿Ñ€Ð¾Ñ„Ð¸Ð»Ñ

---

## Overview

Implemented runtime auto-tuning system for soak tests that automatically adjusts trading profile parameters between iterations based on EDGE_REPORT metrics. Includes safety guardrails, limits enforcement, and detailed tracking.

---

## Components Implemented

### 1. EdgeSentinel Runtime Overrides (`strategy/edge_sentinel.py`)

**Purpose:** Support runtime parameter adjustments on top of loaded profiles.

**Key Features:**
- âœ… Read overrides from `MM_RUNTIME_OVERRIDES_JSON` env var or `artifacts/soak/runtime_overrides.json` file
- âœ… Apply overrides with limits enforcement
- âœ… Track all adjustments with timestamps and reasons
- âœ… Save comprehensive state to `applied_profile.json`

**Runtime Limits:**
```python
{
    "min_interval_ms": (50, 300),
    "replace_rate_per_min": (120, 360),
    "base_spread_bps_delta": (0.0, 0.6),
    "impact_cap_ratio": (0.04, 0.12),
    "tail_age_ms": (400, 1000),
}
```

**New Methods:**
- `load_runtime_overrides()` â€” Load from ENV/file
- `apply_runtime_overrides(overrides)` â€” Apply with limits
- `track_runtime_adjustment(field, from, to, reason)` â€” Track history

**Output Structure (`applied_profile.json`):**
```json
{
  "profile": "S1",
  "base": {...},
  "overrides_runtime": {
    "min_interval_ms": 80,
    "replace_rate_per_min": 330
  },
  "runtime_adjustments": [
    {
      "ts": "2025-10-12T14:00:00Z",
      "field": "min_interval_ms",
      "from": 60,
      "to": 80,
      "reason": "cancel_ratio>0.55"
    }
  ],
  "applied": {...}
}
```

**Markers:**
```
| runtime_overrides | OK | SOURCE=file |
| runtime_adjust | OK | FIELD=min_interval_ms FROM=60 TO=80 REASON=cancel_ratio>0.55 |
```

---

### 2. Auto-Tuning Logic (`tools/soak/run.py`)

**Purpose:** Compute and apply parameter adjustments between soak iterations.

**Triggers:**

| Condition | Adjustment | Reason Tag |
|-----------|-----------|------------|
| `cancel_ratio > 0.55` | `min_interval_ms +20`, `replace_rate_per_min -30` | `cancel_ratio>0.55` |
| `adverse_bps_p95 > 4` OR `slippage_bps_p95 > 3` | `base_spread_bps_delta +0.05` | `adverse/slippage>threshold` |
| `order_age_p95_ms > 330` | `replace_rate_per_min -30`, `tail_age_ms +50` | `order_age>330` |
| `ws_lag_p95_ms > 120` | `min_interval_ms +20` | `ws_lag>120` |
| `net_bps < 2.5` (only if no other triggers) | `base_spread_bps_delta +0.02` | `net_bps<2.5` |

**Guardrails:**

1. **Max 2 Changes Per Field Per Iteration:**
   - Prevents oscillation
   - Each field can only be adjusted twice per iteration

2. **Multi-Fail Guard:**
   - Triggered when 3+ independent failure reasons occur
   - Only allows "calming" adjustments:
     - â†‘ `base_spread_bps_delta`
     - â†‘ `min_interval_ms`
     - â†“ `replace_rate_per_min`
   - Marker: `| soak_iter_tune | SKIP | REASON=multi_fail_guard |`

3. **Spread Delta Cap:**
   - Max total change of `base_spread_bps_delta` per iteration: 0.1
   - Prevents aggressive spread widening

4. **Limits Enforcement:**
   - All adjustments clamped to min/max values
   - Prevents extreme configurations

**New Functions:**
- `load_edge_report(path)` â€” Load EDGE_REPORT.json
- `compute_tuning_adjustments(edge_report, current_overrides)` â€” Compute new overrides
- `save_runtime_overrides(overrides, path)` â€” Save to file

**CLI:**
```bash
python -m tools.soak.run \
    --iterations 10 \
    --mock \
    --auto-tune
```

**Iteration Flow:**
1. Load current `runtime_overrides.json` (if exists)
2. Apply overrides to sentinel
3. Run strategy iteration (simulated in mock mode)
4. Generate `EDGE_REPORT.json` with metrics
5. Compute tuning adjustments based on triggers
6. Save new `runtime_overrides.json`
7. Repeat for next iteration

**Output Markers:**
```
| soak_iter_tune | OK | ADJUSTMENTS=2 net_bps=2.62 cancel=0.48 age_p95=312 lag_p95=90 |
  - cancel_ratio>0.55
  - ws_lag>120

| soak_iter_tune | OK | ADJUSTMENTS=0 metrics_stable |

| soak_iter_tune | SKIP | REASON=multi_fail_guard |
```

---

## Tests

### Unit Tests (`tests/unit/test_runtime_tuning.py`) â€” 11 tests

- âœ… `test_trigger_cancel_ratio` â€” Verify cancel_ratio trigger
- âœ… `test_trigger_adverse_slippage` â€” Verify adverse/slippage trigger
- âœ… `test_trigger_order_age` â€” Verify order_age trigger
- âœ… `test_trigger_ws_lag` â€” Verify ws_lag trigger
- âœ… `test_trigger_net_bps_low` â€” Verify net_bps trigger
- âœ… `test_limits_enforcement` â€” Verify limits are enforced
- âœ… `test_multi_fail_guard` â€” Verify multi-fail guard
- âœ… `test_spread_delta_cap` â€” Verify spread delta cap
- âœ… `test_max_two_changes_per_field` â€” Verify 2-changes limit
- âœ… `test_no_triggers` â€” Verify no adjustments when metrics good
- âœ… `test_incremental_adjustment` â€” Verify adjustments build on previous

### E2E Tests (`tests/e2e/test_soak_autotune_dry.py`) â€” 4 tests

- âœ… `test_soak_autotune_mock_3_iterations` â€” Full 3-iteration simulation
- âœ… `test_soak_autotune_without_flag` â€” Verify flag is required
- âœ… `test_soak_autotune_with_profile_s1` â€” Verify S1 profile integration
- âœ… `test_soak_autotune_markers_and_structure` â€” Verify markers and JSON structure

**All tests PASSED** (15 total) âœ…

---

## Usage Examples

### Mini-Soak with Auto-Tuning (Mock)
```bash
MM_PROFILE=S1 python -m tools.soak.run \
    --iterations 3 \
    --mock \
    --auto-tune
```

**Output:**
```
[INFO] Loading profile: S1
| profile_apply | OK | PROFILE=S1 |
| save_applied_profile | OK | artifacts/soak/applied_profile.json |
[INFO] Profile S1 applied successfully
[INFO] Running mini-soak with auto-tuning: 3 iterations

============================================================
[ITER 1/3] Starting iteration
============================================================
| soak_iter_tune | SKIP | REASON=multi_fail_guard |

============================================================
[ITER 2/3] Starting iteration
============================================================
| runtime_overrides | OK | SOURCE=file |
| save_applied_profile | OK | artifacts/soak/applied_profile.json |
| soak_iter_tune | OK | ADJUSTMENTS=0 metrics_stable |

============================================================
[MINI-SOAK COMPLETE] 3 iterations with auto-tuning
============================================================
Final overrides: {
  "base_spread_bps_delta": 0.05,
  "min_interval_ms": 80,
  "replace_rate_per_min": 300
}
```

### Staging Soak (6h, No Secrets)
```bash
MM_PROFILE=S1 \
MM_ALLOW_MISSING_SECRETS=1 \
python -m tools.soak.run \
    --hours 6 \
    --auto-tune
```

### Production Soak (24-72h, With Secrets)
```bash
MM_PROFILE=S1 \
python -m tools.soak.run \
    --hours 24 \
    --auto-tune
```

---

## Manual Override

You can manually set runtime overrides via ENV:

```bash
export MM_RUNTIME_OVERRIDES_JSON='{"min_interval_ms":100,"replace_rate_per_min":250}'

MM_PROFILE=S1 python -m tools.soak.run --iterations 5 --auto-tune
```

Or via file:

```bash
# Create override file
cat > artifacts/soak/runtime_overrides.json << 'EOF'
{
  "min_interval_ms": 100,
  "replace_rate_per_min": 250,
  "base_spread_bps_delta": 0.1
}
EOF

MM_PROFILE=S1 python -m tools.soak.run --iterations 5 --auto-tune
```

---

## Expected Outcomes

After implementing auto-tuning, expected metrics:

| Metric | Target | Auto-Tuning Behavior |
|--------|--------|---------------------|
| `total.net_bps` | â‰¥ 2.5 | â†‘ spread if low |
| `cancel_ratio` | â‰¤ 0.55 | â†‘ min_interval, â†“ replace_rate |
| `order_age_p95` | â‰¤ 330 ms | â†“ replace_rate, â†‘ tail_age |
| `maker_share` | â‰¥ 85% | Monitor, adjust via spread |
| `adverse_bps_p95` | â‰¤ 4.0 | â†‘ spread |
| `ws_lag_p95_ms` | â‰¤ 120 ms | â†‘ min_interval |

**Convergence:** Auto-tuning should stabilize metrics within 3-5 iterations in most cases.

---

## Files Modified/Created

### Modified
- âœ… `strategy/edge_sentinel.py` â€” Added runtime overrides support
- âœ… `tools/soak/run.py` â€” Added auto-tuning logic and --auto-tune flag

### Created
- âœ… `tests/unit/test_runtime_tuning.py` â€” Unit tests (11 tests)
- âœ… `tests/e2e/test_soak_autotune_dry.py` â€” E2E tests (4 tests)
- âœ… `RUNTIME_AUTOTUNING_IMPLEMENTATION.md` â€” This documentation

---

## Key Features

âœ… **On-the-fly adjustment** â€” Parameters adjusted between iterations without restart  
âœ… **Safe guardrails** â€” Multi-fail guard, 2-changes limit, spread delta cap  
âœ… **Limits enforcement** â€” All adjustments clamped to safe ranges  
âœ… **Detailed tracking** â€” Full history in `applied_profile.json`  
âœ… **Stable markers** â€” CI/CD-friendly output markers  
âœ… **Comprehensive tests** â€” 15 tests (11 unit + 4 e2e), all PASSED  
âœ… **Mock mode** â€” Test auto-tuning without live trading  
âœ… **Profile integration** â€” Works seamlessly with S1 profile system  
âœ… **No dependencies** â€” Stdlib-only implementation  

---

## Acceptance Criteria

âœ… `--auto-tune` flag works and creates `runtime_overrides.json`  
âœ… `applied_profile.json` updated with runtime_adjustments  
âœ… Markers printed for each tuning decision  
âœ… Limits and guardrails enforced  
âœ… All tests PASS (15/15)  
âœ… Mock mode generates problematic then improving metrics  
âœ… Multi-fail guard prevents aggressive adjustments  
âœ… Adjustments build incrementally across iterations  

---

## Integration with Prompts A-C

**Prompt A (Profile S1):** Auto-tuning builds on S1 profile parameters  
**Prompt B (Safe Mode):** Auto-tuning works in safe mode without secrets  
**Prompt C (Extended EDGE_REPORT):** Auto-tuning uses extended metrics (P95, ratios)  

**Combined Flow:**
1. Load S1 profile (`MM_PROFILE=S1`)
2. Run soak with auto-tuning (`--auto-tune`)
3. Generate extended EDGE_REPORT (Prompt C)
4. Apply runtime adjustments (Prompt D)
5. Validate with KPI Gate (Prompt C)

---

## Summary

Successfully implemented runtime auto-tuning for soak tests with:
- âœ… 5 trigger conditions
- âœ… 4 safety guardrails
- âœ… 5 tunable parameters
- âœ… Comprehensive tracking
- âœ… 15 tests, all PASSED

**Ready for production use** ðŸš€

