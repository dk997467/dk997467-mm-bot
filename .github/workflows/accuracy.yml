name: Accuracy Gate (Shadow ‚Üî Dry-Run)

on:
  pull_request:
    paths:
      - 'tools/shadow/**'
      - 'tools/dryrun/**'
      - 'tools/accuracy/**'
      - '.github/workflows/accuracy.yml'
  workflow_dispatch:
    inputs:
      min_windows:
        description: 'Minimum windows required'
        required: false
        default: '24'
        type: string
      mape_threshold:
        description: 'MAPE threshold (0.15 = 15%)'
        required: false
        default: '0.15'
        type: string

concurrency:
  group: accuracy-${{ github.ref }}
  cancel-in-progress: true

jobs:
  accuracy-gate:
    name: Accuracy Comparison
    runs-on: ubuntu-latest
    timeout-minutes: 30
    env:
      PYTEST_DISABLE_PLUGIN_AUTOLOAD: "1"
      PYTHONPATH: "${{ github.workspace }}"
      BYBIT_API_KEY: "test_api_key_for_ci_only"
      BYBIT_API_SECRET: "test_api_secret_for_ci_only"
      STORAGE_PG_PASSWORD: "test_pg_password_for_ci_only"
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        with:
          lfs: true
      
      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'
          cache: 'pip'
      
      - name: Install Rust toolchain
        uses: actions-rust-lang/setup-rust-toolchain@v1
        with:
          toolchain: stable
      
      - name: Prepare minimal requirements for CI
        shell: bash
        run: |
          awk '
            BEGIN{IGNORECASE=1}
            /^[[:space:]]*bybit-connector/ {next}
            /^[[:space:]]*mm-orderbook/ {next}
            /^[[:space:]]*mm_orderbook/ {next}
            /^[[:space:]]*git\+/ {next}
            {print}
          ' requirements.txt > requirements_ci.txt
          
          echo "pydantic>=2,<3" >> requirements_ci.txt
          echo "pydantic-settings>=2,<3" >> requirements_ci.txt
          echo "pandas>=2,<3" >> requirements_ci.txt
      
      - name: Install dependencies
        run: |
          python -m pip install -U pip
          pip install maturin
          pip install -e . -v
          pip install -r requirements_ci.txt
      
      - name: Run Shadow Mode (mock, 24 iterations)
        shell: bash
        run: |
          echo "================================================"
          echo "SHADOW MODE: 24 iterations for accuracy baseline"
          echo "================================================"
          
          mkdir -p artifacts/shadow/latest
          
          python -m tools.shadow.run_shadow \
            --iterations 24 \
            --duration 60 \
            --profile moderate \
            --exchange bybit \
            --mock \
            --output artifacts/shadow/latest
          
          if [ $? -ne 0 ]; then
            echo "‚ùå Shadow run failed"
            exit 1
          fi
          
          echo "‚úì Shadow run completed"
          ls -lah artifacts/shadow/latest/ITER_SUMMARY_*.json | head -5
      
      - name: Run Dry-Run Mode (mock/redis, 24 iterations)
        shell: bash
        run: |
          echo "================================================"
          echo "DRY-RUN MODE: 24 iterations for accuracy comparison"
          echo "================================================"
          
          mkdir -p artifacts/dryrun/latest
          
          # Run dryrun with same parameters as shadow
          python -m tools.dryrun.run_dryrun \
            --symbols BTCUSDT ETHUSDT \
            --iterations 24 \
            --duration 60 \
            --output artifacts/dryrun/latest
          
          if [ $? -ne 0 ]; then
            echo "‚ùå Dry-run failed"
            exit 1
          fi
          
          echo "‚úì Dry-run completed"
          ls -lah artifacts/dryrun/latest/ITER_SUMMARY_*.json | head -5
      
      - name: Run Accuracy Comparison
        id: accuracy_compare
        continue-on-error: true
        shell: bash
        run: |
          echo "================================================"
          echo "ACCURACY GATE: Comparing Shadow vs Dry-Run"
          echo "================================================"
          
          MIN_WINDOWS="${{ github.event.inputs.min_windows || '24' }}"
          MAPE_THRESHOLD="${{ github.event.inputs.mape_threshold || '0.15' }}"
          
          python -m tools.accuracy.compare_shadow_dryrun \
            --shadow "artifacts/shadow/latest/ITER_SUMMARY_*.json" \
            --dryrun "artifacts/dryrun/latest/ITER_SUMMARY_*.json" \
            --symbols BTCUSDT,ETHUSDT \
            --min-windows "$MIN_WINDOWS" \
            --max-age-min 120 \
            --mape-threshold "$MAPE_THRESHOLD" \
            --median-delta-threshold-bps 1.5 \
            --out-dir reports/analysis \
            --verbose
          
          EXIT_CODE=$?
          echo "exit_code=$EXIT_CODE" >> $GITHUB_OUTPUT
          
          echo ""
          echo "Accuracy comparison exit code: $EXIT_CODE"
          echo "  0 = PASS, 1 = FAIL, 2 = WARN"
      
      - name: Display Accuracy Report
        if: always()
        shell: bash
        run: |
          echo "================================================"
          echo "ACCURACY REPORT"
          echo "================================================"
          
          if [ -f reports/analysis/ACCURACY_REPORT.md ]; then
            cat reports/analysis/ACCURACY_REPORT.md
          else
            echo "‚ùå ACCURACY_REPORT.md not found"
          fi
          
          echo ""
          echo "================================================"
          echo "ACCURACY SUMMARY (JSON)"
          echo "================================================"
          
          if [ -f reports/analysis/ACCURACY_SUMMARY.json ]; then
            cat reports/analysis/ACCURACY_SUMMARY.json | jq .
          else
            echo "‚ùå ACCURACY_SUMMARY.json not found"
          fi
      
      - name: Upload Accuracy Artifacts
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: accuracy-gate-${{ github.run_id }}
          path: |
            reports/analysis/ACCURACY_*.md
            reports/analysis/ACCURACY_*.json
            artifacts/shadow/latest/ITER_SUMMARY_*.json
            artifacts/dryrun/latest/ITER_SUMMARY_*.json
          if-no-files-found: warn
          retention-days: 30
      
      - name: Comment Accuracy Results to PR
        if: always() && github.event_name == 'pull_request' && github.event.pull_request.number
        uses: actions/github-script@v7
        with:
          github-token: ${{ secrets.GITHUB_TOKEN }}
          script: |
            const fs = require('fs');
            const summaryPath = 'reports/analysis/ACCURACY_SUMMARY.json';
            
            if (!fs.existsSync(summaryPath)) {
              console.log('ACCURACY_SUMMARY.json not found, skipping comment');
              return;
            }
            
            const summary = JSON.parse(fs.readFileSync(summaryPath, 'utf8'));
            const verdict = summary.verdict || 'UNKNOWN';
            const badge = verdict === 'PASS' ? '‚úÖ' : (verdict === 'WARN' ? 'üü°' : 'üî¥');
            const thresholds = summary.thresholds || {};
            
            // Build symbol table
            let table = `| Symbol | KPI | MAPE (%) | Median Œî | Status |\n|--------|-----|----------|----------|--------|\n`;
            
            for (const [sym, kpis] of Object.entries(summary.symbols || {})) {
              for (const [kpi, metrics] of Object.entries(kpis)) {
                const mape = metrics.mape_pct ?? 'n/a';
                const delta = metrics.median_delta ?? 'n/a';
                const status = metrics.status || 'OK';
                const statusBadge = status === 'OK' ? '‚úÖ' : (status === 'WARN' ? 'üü°' : 'üî¥');
                
                table += `| ${sym} | ${kpi} | ${mape} | ${delta} | ${statusBadge} ${status} |\n`;
              }
            }
            
            // Build artifact links
            const runId = context.runId;
            const artifactsPage = `https://github.com/${context.repo.owner}/${context.repo.repo}/actions/runs/${runId}#artifacts`;
            
            const comment = `### ${badge} Accuracy Gate: ${verdict}
            
            **Shadow ‚Üî Dry-Run Comparison**
            
            ${table}
            
            **Thresholds:**
            - MAPE: **${thresholds.mape_pct ?? 'n/a'}%**
            - Median Œî (BPS): **${thresholds.median_delta_bps ?? 'n/a'}**
            
            **Summary:**
            - Symbols: **${summary.meta?.symbols_count ?? 0}**
            - Fail count: **${summary.meta?.fail_count ?? 0}**
            - Warn count: **${summary.meta?.warn_count ?? 0}**
            
            **Artifacts:** [üì¶ Download](${artifactsPage})
            
            _Generated: ${summary.generated_at_utc || 'n/a'}_`;
            
            await github.rest.issues.createComment({
              owner: context.repo.owner,
              repo: context.repo.repo,
              issue_number: context.payload.pull_request.number,
              body: comment
            });
      
      - name: Check Accuracy Gate Result
        if: always()
        shell: bash
        run: |
          EXIT_CODE="${{ steps.accuracy_compare.outputs.exit_code }}"
          
          echo "================================================"
          echo "ACCURACY GATE RESULT"
          echo "================================================"
          
          if [ "$EXIT_CODE" = "0" ]; then
            echo "‚úÖ Accuracy Gate PASSED"
            echo "Shadow and Dry-Run KPIs are within acceptable thresholds"
            exit 0
          elif [ "$EXIT_CODE" = "2" ]; then
            echo "üü° Accuracy Gate WARN"
            echo "Some soft thresholds violated - informational only"
            exit 0
          else
            echo "üî¥ Accuracy Gate FAILED"
            echo "Critical thresholds violated - blocking PR"
            echo ""
            echo "Review ACCURACY_REPORT.md for details"
            exit 1
          fi

